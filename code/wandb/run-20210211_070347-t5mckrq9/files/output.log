02/11/2021 07:03:48 - WARNING - __main__ -   device: cuda, n_gpu: 1, 16-bits training: False
02/11/2021 07:03:48 - INFO - __main__ -   CODES ATTENTION IS True
02/11/2021 07:03:48 - INFO - __main__ -   INCLUDE CODES IS True
02/11/2021 07:03:48 - INFO - __main__ -   PRETRAINED ICD IS True
02/11/2021 07:03:48 - INFO - __main__ -   ONLY CODES IS False
Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at xhlu/electra-medal and are newly initialized: ['encoder.layer.0.attention.graph_projection.weight', 'encoder.layer.0.attention.graph_projection.bias', 'encoder.layer.1.attention.graph_projection.weight', 'encoder.layer.1.attention.graph_projection.bias', 'encoder.layer.2.attention.graph_projection.weight', 'encoder.layer.2.attention.graph_projection.bias', 'encoder.layer.3.attention.graph_projection.weight', 'encoder.layer.3.attention.graph_projection.bias', 'encoder.layer.4.attention.graph_projection.weight', 'encoder.layer.4.attention.graph_projection.bias', 'encoder.layer.5.attention.graph_projection.weight', 'encoder.layer.5.attention.graph_projection.bias', 'encoder.layer.6.attention.graph_projection.weight', 'encoder.layer.6.attention.graph_projection.bias', 'encoder.layer.7.attention.graph_projection.weight', 'encoder.layer.7.attention.graph_projection.bias', 'encoder.layer.8.attention.graph_projection.weight', 'encoder.layer.8.attention.graph_projection.bias', 'encoder.layer.9.attention.graph_projection.weight', 'encoder.layer.9.attention.graph_projection.bias', 'encoder.layer.10.attention.graph_projection.weight', 'encoder.layer.10.attention.graph_projection.bias', 'encoder.layer.11.attention.graph_projection.weight', 'encoder.layer.11.attention.graph_projection.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
02/11/2021 07:03:49 - INFO - __main__ -   we have added 1067 tokens
electra.embeddings.word_embeddings.weight 02/11/2021 07:03:52 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir=None, codes_attention=True, codes_learning_rate=1e-05, config_name='', data_dir='/home/dc925/project/data/graphmimic/mortality/full_chunked', device=device(type='cuda'), do_eval=True, do_lower_case=False, do_test=True, do_train=True, eval_all_checkpoints=False, eval_steps=500, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, include_codes=True, learning_rate=1e-05, logging_steps=10, max_grad_norm=1.0, max_seq_length=512, max_steps=-1, model_name_or_path='xhlu/electra-medal', model_type='electra', n_gpu=1, no_cuda=False, num_train_epochs=4.0, only_codes=False, output_dir='/home/dc925/project/output/209/kge', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=16, per_gpu_train_batch_size=16, pretrained_icd=True, save_steps=0, seed=42, task='mortality', task_name='mortality', threshold=0.5, tokenizer_name='', warmup_steps=0, weight_decay=0.0)
torch.Size([31589, 128]) 4043392
electra.embeddings.position_embeddings.weight torch.Size([512, 128]) 65536
electra.embeddings.token_type_embeddings.weight torch.Size([2, 128]) 256
electra.embeddings.LayerNorm.weight torch.Size([128]) 128
electra.embeddings.LayerNorm.bias torch.Size([128]) 128
electra.embeddings_project.weight torch.Size([256, 128]) 32768
electra.embeddings_project.bias torch.Size([256]) 256
electra.encoder.layer.0.attention.self.query.weight torch.Size([256, 256]) 65536
electra.encoder.layer.0.attention.self.query.bias torch.Size([256]) 256
electra.encoder.layer.0.attention.self.key.weight torch.Size([256, 256]) 65536
electra.encoder.layer.0.attention.self.key.bias torch.Size([256]) 256
electra.encoder.layer.0.attention.self.value.weight torch.Size([256, 256]) 65536
electra.encoder.layer.0.attention.self.value.bias torch.Size([256]) 256
electra.encoder.layer.0.attention.output.dense.weight torch.Size([256, 256]) 65536
electra.encoder.layer.0.attention.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.0.attention.graph_projection.weight torch.Size([256, 256]) 65536
electra.encoder.layer.0.attention.graph_projection.bias torch.Size([256]) 256
electra.encoder.layer.0.intermediate.dense.weight torch.Size([1024, 256]) 262144
electra.encoder.layer.0.intermediate.dense.bias torch.Size([1024]) 1024
electra.encoder.layer.0.output.dense.weight torch.Size([256, 1024]) 262144
electra.encoder.layer.0.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.0.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.0.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.1.attention.self.query.weight torch.Size([256, 256]) 65536
electra.encoder.layer.1.attention.self.query.bias torch.Size([256]) 256
electra.encoder.layer.1.attention.self.key.weight torch.Size([256, 256]) 65536
electra.encoder.layer.1.attention.self.key.bias torch.Size([256]) 256
electra.encoder.layer.1.attention.self.value.weight torch.Size([256, 256]) 65536
electra.encoder.layer.1.attention.self.value.bias torch.Size([256]) 256
electra.encoder.layer.1.attention.output.dense.weight torch.Size([256, 256]) 65536
electra.encoder.layer.1.attention.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.1.attention.graph_projection.weight torch.Size([256, 256]) 65536
electra.encoder.layer.1.attention.graph_projection.bias torch.Size([256]) 256
electra.encoder.layer.1.intermediate.dense.weight torch.Size([1024, 256]) 262144
electra.encoder.layer.1.intermediate.dense.bias torch.Size([1024]) 1024
electra.encoder.layer.1.output.dense.weight torch.Size([256, 1024]) 262144
electra.encoder.layer.1.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.1.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.1.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.2.attention.self.query.weight torch.Size([256, 256]) 65536
electra.encoder.layer.2.attention.self.query.bias torch.Size([256]) 256
electra.encoder.layer.2.attention.self.key.weight torch.Size([256, 256]) 65536
electra.encoder.layer.2.attention.self.key.bias torch.Size([256]) 256
electra.encoder.layer.2.attention.self.value.weight torch.Size([256, 256]) 65536
electra.encoder.layer.2.attention.self.value.bias torch.Size([256]) 256
electra.encoder.layer.2.attention.output.dense.weight torch.Size([256, 256]) 65536
electra.encoder.layer.2.attention.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.2.attention.graph_projection.weight torch.Size([256, 256]) 65536
electra.encoder.layer.2.attention.graph_projection.bias torch.Size([256]) 256
electra.encoder.layer.2.intermediate.dense.weight torch.Size([1024, 256]) 262144
electra.encoder.layer.2.intermediate.dense.bias torch.Size([1024]) 1024
electra.encoder.layer.2.output.dense.weight torch.Size([256, 1024]) 262144
electra.encoder.layer.2.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.2.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.2.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.3.attention.self.query.weight torch.Size([256, 256]) 65536
electra.encoder.layer.3.attention.self.query.bias torch.Size([256]) 256
electra.encoder.layer.3.attention.self.key.weight torch.Size([256, 256]) 65536
electra.encoder.layer.3.attention.self.key.bias torch.Size([256]) 256
electra.encoder.layer.3.attention.self.value.weight torch.Size([256, 256]) 65536
electra.encoder.layer.3.attention.self.value.bias torch.Size([256]) 256
electra.encoder.layer.3.attention.output.dense.weight torch.Size([256, 256]) 65536
electra.encoder.layer.3.attention.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.3.attention.graph_projection.weight torch.Size([256, 256]) 65536
electra.encoder.layer.3.attention.graph_projection.bias torch.Size([256]) 256
electra.encoder.layer.3.intermediate.dense.weight torch.Size([1024, 256]) 262144
electra.encoder.layer.3.intermediate.dense.bias torch.Size([1024]) 1024
electra.encoder.layer.3.output.dense.weight torch.Size([256, 1024]) 262144
electra.encoder.layer.3.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.3.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.3.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.4.attention.self.query.weight torch.Size([256, 256]) 65536
electra.encoder.layer.4.attention.self.query.bias torch.Size([256]) 256
electra.encoder.layer.4.attention.self.key.weight torch.Size([256, 256]) 65536
electra.encoder.layer.4.attention.self.key.bias torch.Size([256]) 256
electra.encoder.layer.4.attention.self.value.weight torch.Size([256, 256]) 65536
electra.encoder.layer.4.attention.self.value.bias torch.Size([256]) 256
electra.encoder.layer.4.attention.output.dense.weight torch.Size([256, 256]) 65536
electra.encoder.layer.4.attention.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.4.attention.graph_projection.weight torch.Size([256, 256]) 65536
electra.encoder.layer.4.attention.graph_projection.bias torch.Size([256]) 256
electra.encoder.layer.4.intermediate.dense.weight torch.Size([1024, 256]) 262144
electra.encoder.layer.4.intermediate.dense.bias torch.Size([1024]) 1024
electra.encoder.layer.4.output.dense.weight torch.Size([256, 1024]) 262144
electra.encoder.layer.4.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.4.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.4.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.5.attention.self.query.weight torch.Size([256, 256]) 65536
electra.encoder.layer.5.attention.self.query.bias torch.Size([256]) 256
electra.encoder.layer.5.attention.self.key.weight torch.Size([256, 256]) 65536
electra.encoder.layer.5.attention.self.key.bias torch.Size([256]) 256
electra.encoder.layer.5.attention.self.value.weight torch.Size([256, 256]) 65536
electra.encoder.layer.5.attention.self.value.bias torch.Size([256]) 256
electra.encoder.layer.5.attention.output.dense.weight torch.Size([256, 256]) 65536
electra.encoder.layer.5.attention.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.5.attention.graph_projection.weight torch.Size([256, 256]) 65536
electra.encoder.layer.5.attention.graph_projection.bias torch.Size([256]) 256
electra.encoder.layer.5.intermediate.dense.weight torch.Size([1024, 256]) 262144
electra.encoder.layer.5.intermediate.dense.bias torch.Size([1024]) 1024
electra.encoder.layer.5.output.dense.weight torch.Size([256, 1024]) 262144
electra.encoder.layer.5.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.5.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.5.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.6.attention.self.query.weight torch.Size([256, 256]) 65536
electra.encoder.layer.6.attention.self.query.bias torch.Size([256]) 256
electra.encoder.layer.6.attention.self.key.weight torch.Size([256, 256]) 65536
electra.encoder.layer.6.attention.self.key.bias torch.Size([256]) 256
electra.encoder.layer.6.attention.self.value.weight torch.Size([256, 256]) 65536
electra.encoder.layer.6.attention.self.value.bias torch.Size([256]) 256
electra.encoder.layer.6.attention.output.dense.weight torch.Size([256, 256]) 65536
electra.encoder.layer.6.attention.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.6.attention.graph_projection.weight torch.Size([256, 256]) 65536
electra.encoder.layer.6.attention.graph_projection.bias torch.Size([256]) 256
electra.encoder.layer.6.intermediate.dense.weight torch.Size([1024, 256]) 262144
electra.encoder.layer.6.intermediate.dense.bias torch.Size([1024]) 1024
electra.encoder.layer.6.output.dense.weight torch.Size([256, 1024]) 262144
electra.encoder.layer.6.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.6.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.6.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.7.attention.self.query.weight torch.Size([256, 256]) 65536
electra.encoder.layer.7.attention.self.query.bias torch.Size([256]) 256
electra.encoder.layer.7.attention.self.key.weight torch.Size([256, 256]) 65536
electra.encoder.layer.7.attention.self.key.bias torch.Size([256]) 256
electra.encoder.layer.7.attention.self.value.weight torch.Size([256, 256]) 65536
electra.encoder.layer.7.attention.self.value.bias torch.Size([256]) 256
electra.encoder.layer.7.attention.output.dense.weight torch.Size([256, 256]) 65536
electra.encoder.layer.7.attention.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.7.attention.graph_projection.weight torch.Size([256, 256]) 65536
electra.encoder.layer.7.attention.graph_projection.bias torch.Size([256]) 256
electra.encoder.layer.7.intermediate.dense.weight torch.Size([1024, 256]) 262144
electra.encoder.layer.7.intermediate.dense.bias torch.Size([1024]) 1024
electra.encoder.layer.7.output.dense.weight torch.Size([256, 1024]) 262144
electra.encoder.layer.7.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.7.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.7.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.8.attention.self.query.weight torch.Size([256, 256]) 65536
electra.encoder.layer.8.attention.self.query.bias torch.Size([256]) 256
electra.encoder.layer.8.attention.self.key.weight torch.Size([256, 256]) 65536
electra.encoder.layer.8.attention.self.key.bias torch.Size([256]) 256
electra.encoder.layer.8.attention.self.value.weight torch.Size([256, 256]) 65536
electra.encoder.layer.8.attention.self.value.bias torch.Size([256]) 256
electra.encoder.layer.8.attention.output.dense.weight torch.Size([256, 256]) 65536
electra.encoder.layer.8.attention.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.8.attention.graph_projection.weight torch.Size([256, 256]) 65536
electra.encoder.layer.8.attention.graph_projection.bias torch.Size([256]) 256
electra.encoder.layer.8.intermediate.dense.weight torch.Size([1024, 256]) 262144
electra.encoder.layer.8.intermediate.dense.bias torch.Size([1024]) 1024
electra.encoder.layer.8.output.dense.weight torch.Size([256, 1024]) 262144
electra.encoder.layer.8.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.8.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.8.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.9.attention.self.query.weight torch.Size([256, 256]) 65536
electra.encoder.layer.9.attention.self.query.bias torch.Size([256]) 256
electra.encoder.layer.9.attention.self.key.weight torch.Size([256, 256]) 65536
electra.encoder.layer.9.attention.self.key.bias torch.Size([256]) 256
electra.encoder.layer.9.attention.self.value.weight torch.Size([256, 256]) 65536
electra.encoder.layer.9.attention.self.value.bias torch.Size([256]) 256
electra.encoder.layer.9.attention.output.dense.weight torch.Size([256, 256]) 65536
electra.encoder.layer.9.attention.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.9.attention.graph_projection.weight torch.Size([256, 256]) 65536
electra.encoder.layer.9.attention.graph_projection.bias torch.Size([256]) 256
electra.encoder.layer.9.intermediate.dense.weight torch.Size([1024, 256]) 262144
electra.encoder.layer.9.intermediate.dense.bias torch.Size([1024]) 1024
electra.encoder.layer.9.output.dense.weight torch.Size([256, 1024]) 262144
electra.encoder.layer.9.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.9.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.9.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.10.attention.self.query.weight torch.Size([256, 256]) 65536
electra.encoder.layer.10.attention.self.query.bias torch.Size([256]) 256
electra.encoder.layer.10.attention.self.key.weight torch.Size([256, 256]) 65536
electra.encoder.layer.10.attention.self.key.bias torch.Size([256]) 256
electra.encoder.layer.10.attention.self.value.weight torch.Size([256, 256]) 65536
electra.encoder.layer.10.attention.self.value.bias torch.Size([256]) 256
electra.encoder.layer.10.attention.output.dense.weight torch.Size([256, 256]) 65536
electra.encoder.layer.10.attention.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.10.attention.graph_projection.weight torch.Size([256, 256]) 65536
electra.encoder.layer.10.attention.graph_projection.bias torch.Size([256]) 256
electra.encoder.layer.10.intermediate.dense.weight torch.Size([1024, 256]) 02/11/2021 07:03:52 - INFO - __main__ -   loading features from cached file /home/dc925/project/data/graphmimic/mortality/full_chunked/cached_train_electra-medal_mortality_512_codes
262144
electra.encoder.layer.10.intermediate.dense.bias torch.Size([1024]) 1024
electra.encoder.layer.10.output.dense.weight torch.Size([256, 1024]) 262144
electra.encoder.layer.10.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.10.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.10.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.11.attention.self.query.weight torch.Size([256, 256]) 65536
electra.encoder.layer.11.attention.self.query.bias torch.Size([256]) 256
electra.encoder.layer.11.attention.self.key.weight torch.Size([256, 256]) 65536
electra.encoder.layer.11.attention.self.key.bias torch.Size([256]) 256
electra.encoder.layer.11.attention.self.value.weight torch.Size([256, 256]) 65536
electra.encoder.layer.11.attention.self.value.bias torch.Size([256]) 256
electra.encoder.layer.11.attention.output.dense.weight torch.Size([256, 256]) 65536
electra.encoder.layer.11.attention.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([256]) 256
electra.encoder.layer.11.attention.graph_projection.weight torch.Size([256, 256]) 65536
electra.encoder.layer.11.attention.graph_projection.bias torch.Size([256]) 256
electra.encoder.layer.11.intermediate.dense.weight torch.Size([1024, 256]) 262144
electra.encoder.layer.11.intermediate.dense.bias torch.Size([1024]) 1024
electra.encoder.layer.11.output.dense.weight torch.Size([256, 1024]) 262144
electra.encoder.layer.11.output.dense.bias torch.Size([256]) 256
electra.encoder.layer.11.output.LayerNorm.weight torch.Size([256]) 256
electra.encoder.layer.11.output.LayerNorm.bias torch.Size([256]) 256
classifier.dense.weight torch.Size([256, 256]) 65536
classifier.dense.bias torch.Size([256]) 256
classifier.out_proj.weight torch.Size([2, 256]) 512
classifier.out_proj.bias torch.Size([2]) 2
14475394
